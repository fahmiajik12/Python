{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahmiajik12/Python/blob/Master/XrayPnumonia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5idX8fHBbZ2"
      },
      "source": [
        "# Pendahuluan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wwhON5gxKzr",
        "outputId": "dd9576b4-ad11-4b93-8e79-49148e014407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.9.24)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "#instalasi kaggel\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "ibk32vTVz3_q",
        "outputId": "4304efe1-2075-4810-b9d3-97c7248f175c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-464712b3-7ff4-42bb-94ce-54c6503b2691\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-464712b3-7ff4-42bb-94ce-54c6503b2691\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRmgd2Ngz9N4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlPiLUbgz-RE"
      },
      "outputs": [],
      "source": [
        "#mengambil dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYO3yy4x0KtU"
      },
      "outputs": [],
      "source": [
        "#mengekstrak dataset pada /content/\n",
        "!unzip chest-xray-pneumonia.zip\n",
        "!rm chest-xray-pneumonia.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJrWjaytZiOB"
      },
      "source": [
        "melakukan import yang diperlukan, selain itu Salah satu praktik terbaik yang harus dilakukan saat melakukan project machine learning adalah dengan menentukan konstanta, sehingga memfasilitasi perubahan lebih lanjut. Mengingat itu, perlu dilakukan penentuan nilai bacth size, tinggi dan lebar gambar, dan *learning rate*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CijyL8Hw973"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from PIL import Image\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import add\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "np.random.seed(777)\n",
        "tf.random.set_seed(777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9XHwJdn1D7v"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 240\n",
        "IMG_WIDTH = 240\n",
        "ALPHA = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVuQeuEaZ7v4"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqUyfZI9aUby"
      },
      "source": [
        "Kernel ini menggunakan dataset Chest X-Ray Images (Pneumonia), yang disusun menjadi 3 folder (train, test, val) dan berisi subfolder untuk setiap kategori gambar (Pneumonia / Normal). Ada 5.863 gambar X-Ray (JPEG) dan 2 kategori (Pneumonia / Normal).\n",
        "\n",
        "Untuk analisis gambar Chest X-Ray, semua radiografi dada pada awalnya diskrining untuk kontrol kualitas dengan menghapus semua pindaian berkualitas rendah atau tidak terbaca. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4O-An1n1ZH8"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/chest_xray'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4dd_n-92tVT"
      },
      "outputs": [],
      "source": [
        "labels = ['NORMAL','PNEUMONIA']\n",
        "def get_data(data_dir):\n",
        "    data = [] \n",
        "    for label in labels: \n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE) \n",
        "                resized_arr = cv2.resize(img_arr, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    return np.array(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaEgBBIMbY-v"
      },
      "source": [
        "kemudian kita definisikan sebuah fungsi untuk mengembalikan np.array dengan semua gambar yang terletak di direktori tertentu dan menggunakannya untuk memuat data training, validation dan test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRB-_Ofd27l9"
      },
      "outputs": [],
      "source": [
        "train = get_data('/content/chest_xray/train/')\n",
        "test = get_data('/content/chest_xray/test/')\n",
        "val = get_data('/content/chest_xray/val/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL2plAUDb4V9"
      },
      "source": [
        "kemudian perlu juga untuk melihat berapa banyak gambar yang dimiliki dari setiap class di training set. Selain itu, mari kita lihat bagaimana gambar didistribusikan diantara training, validation dan test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYCiJhlXbeL8"
      },
      "outputs": [],
      "source": [
        "print(f\"{[y for _, y in train].count(0)} NORMAL IMAGES IN TRAINING SET\")\n",
        "print(f\"{[y for _, y in train].count(1)} PNEUMONIA IMAGES IN TRAINING SET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU5I8vto3izF"
      },
      "outputs": [],
      "source": [
        "print(f'Images in TRAINING SET: {train.shape[0]}')\n",
        "print(f'Images in VALIDATION SET: {val.shape[0]}')\n",
        "print(f'Images in TEST SET: {test.shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohZNC9eYcsDw"
      },
      "source": [
        "Seperti yang dapat dilihat pada output dari dua cell sebelumnya, terdapat masalah data yang tidak seimbang dan dengan proporsi yang agak aneh antara training set dan validation set.\n",
        "\n",
        "masalah tersebut akan coba diatasi dalam tahap pemrosesan data, untuk saat ini yang dilakukan hanya akan menggabungkan dataset train dan val kemudian melakukan pemisahan (split) lagi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWgmkh3A3ix9"
      },
      "outputs": [],
      "source": [
        "train = np.append(train, val, axis=0)\n",
        "train, val = train_test_split(train, test_size=.20, random_state=777)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWBXu7btfGsW"
      },
      "source": [
        "Untuk mengakhiri bagian ini, akan ditampilkan beberapa contoh dalam dataset yang dimiliki."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmZAFoTM305D"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for k, i in np.ndenumerate(np.random.randint(train.shape[0], size=9)):\n",
        "    ax = plt.subplot(3, 3, k[0] + 1)\n",
        "    plt.imshow(train[i][0], cmap='gray')\n",
        "    plt.title(labels[train[i][1]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbNbheKFdjSm"
      },
      "source": [
        "# Processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9AeLL2ufMzH"
      },
      "source": [
        "pada tahap awal ini akan membuat dan menggunakan fungsi yang disebut prepared_data () yang akan menormalkan gambar (membagi setiap piksel dengan 255) dan me-reshape array menjadi sesuai bentuk. Setelah itu, fungsi akan mengembalikan array x dan y secara terpisah dari set kita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIT0kVfP4J-y"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data):\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for feature, label in data:\n",
        "        x.append(feature)\n",
        "        y.append(label)\n",
        "        \n",
        "    x = (np.array(x) / 255).reshape(-1,IMG_WIDTH, IMG_HEIGHT, 1)\n",
        "    y = np.array(y)\n",
        "        \n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = prepare_data(train)\n",
        "x_val, y_val = prepare_data(val)\n",
        "x_test, y_test = prepare_data(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXJBBnmmfs9G"
      },
      "source": [
        "Untuk mencari performa terbaik dari model yang dilakukan, penting untuk menambah jumlah sampel dalam dataset dan untuk itu perlu melakukan proses data augumentasi. Perhatikan bahwa di sini tidak akan menggunakan flip untuk menghasilkan gambar baru karena paru-paru tidak simetris secara horizontal dan vertikal sehingga tidak diperlukan.\n",
        "\n",
        "kemudian juga ditampilkan hasil dari data augmentasi yang dilakukan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSZstXLdeknY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range = 20, \n",
        "    zoom_range = 0.2, \n",
        "    width_shift_range=0.15,  \n",
        "    height_shift_range=0.15,\n",
        "    horizontal_flip = False,  \n",
        "    vertical_flip=False)\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# pick an image to transform\n",
        "image_path = '/content/chest_xray/val/NORMAL/NORMAL2-IM-1427-0001.jpeg'\n",
        "img = image.load_img(image_path)\n",
        "\n",
        "\n",
        "img=image.img_to_array(img)\n",
        "img=img.reshape((1,) + img.shape)\n",
        "\n",
        "i = 0\n",
        "\n",
        "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):  # this loops runs forever until we break, saving images to current directory with specified prefix\n",
        "    plt.figure(i)\n",
        "    plot = plt.imshow(image.img_to_array(batch[0]).astype(np.uint8))\n",
        "    i += 1\n",
        "    if i > 4: \n",
        "        break\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TV2CLPTmfcH"
      },
      "source": [
        "Nah, untuk menyelesaikan masalah ketidakseimbangan data yang disebutkan sebelumnya. Ada beberapa kemungkinan pendekatan untuk diambil tetapi kami akan memilih untuk memberikan weight (bobot/pembebanan) yang berbeda ke kelas.\n",
        "\n",
        "weight ini akan digunakan di pada proses lanjutan sebagai parameter yang sesuai dengan model, dan sebagaimana dijelaskan dalam dokumentasi resmi Keras, fungsi compute_class_weight() dapat berguna untuk memberi tahu model agar \"lebih memperhatikan\" sampel dari under-represented class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXgUvetD76i9"
      },
      "outputs": [],
      "source": [
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(y_train),\n",
        "                                        y = y_train                                                    \n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Membuat Model"
      ],
      "metadata": {
        "id": "l02nEv1IjiG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def block(inputs, filters, stride):\n",
        "    conv_0 = layers.Conv2D(filters=filters, kernel_size=(3, 3), strides=(stride, stride), padding='same', activation='relu')(inputs)\n",
        "    conv_1 = layers.Conv2D(filters=filters, kernel_size=(3, 3), strides=(stride, stride), padding='same', activation='relu')(conv_0)\n",
        "    \n",
        "    skip = layers.Conv2D(input_shape=input_size, filters=filters, kernel_size=(1, 1), strides=(stride**2, stride**2), padding='same', activation='relu')(inputs)\n",
        "    \n",
        "    pool = layers.MaxPool2D(pool_size=(3, 3), strides=(2,2), padding='same')(add([conv_1, skip]))\n",
        "    \n",
        "    return pool"
      ],
      "metadata": {
        "id": "WOKevnqIjvpU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}